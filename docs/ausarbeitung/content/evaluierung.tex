%!TEX root = ../ausarbeitung.tex
\section{Evaluierung}
\label{sec:eval}

In diesem Kapitel widmen wir uns der Evaluierung unserer Ergebnisse. Dabei betrachten wir die Laufzeit des SPM-Verfahrens und die Güte aller Klassifikationsverfahren. Leider liegen für die Segmentierungen keine Ground-Truth-Daten vor, bei denen jedes Pixel als innerhalb oder außerhalb eines Tieres markiert ist. Solche Daten wären aber nötig, um die Lokalisierungstechniken und insbesondere PCA verlässlich testen zu können. Stattdessen haben wir die Technik lediglich subjektiv durch Anwendung auf Bildern getestet. Dort waren die Ergebnisse überzeugend. Des Weiteren fließen die Ergebnisse der PCA-Segmentierung indirekt auch in die Güte der Klassifikationen von SPM ein, weil als gelabelte Trainings- und Testdaten die vorsegmentierten Bilder von PCA verwendet wurden. Wäre die Ergebisse von PCA schlecht, so hätte sich das auch deutlich in der Klassifikation zeigen müssen.

\subsection{Daten}

Als Testdaten liegt uns eine umfangreiche Datenbank von Bildern aus dem niederländischen Nationalpark De Hoge Veluwe vor. Diese umfasst 40GB Bilder von neun verschiedenen Säugetierarten. Die Bildsequenzen wurden mit Reconyx-Kameras erstellt und umfassen zehn bis 120 Bilder. Jedes Bild der Größe 2048 $\times$ 1536 Pixel ist entweder eine farbige Tagaufnahme oder ein infrarotbelichtetes Graustufenbild, das nachts aufgenommen wurde.

Für unsere Tests haben wir zwei verschiedene Datenbanken angelegt. In der Dachs-Damhirsch-Datenbank (DDD) befinden sich alle Tagbilder der Spezies \emph{Meles Meles} (Dachs) und \emph{Dama Dama} (Damhirsch). Für diese liegen jeweils 37 beziehungsweise 262 Bilder vor. \\
Um unserer Verfahren auf komplexeren Datensätzen zu testen, haben wir zusätzlich die Dachs-Damhirsch-Datenbank+ (DDD+) zusammengestellt. Diese umfasst insgesamt 2336 Tag- und Nachtbilder von Rotfuchs, Damhirsch, Wildschwein, Feldhase, Dachs und Wildschaf. Für jede Klasse liegen zwischen 240 und 490 Bilder vor, wobei die Anteile von Tag- und Nachtbildern zwischen den Klassen abweichen. Beim nachtaktiven Dachs liegen beispielsweise kaum Tagbilder vor, während diese beim Wildschaf dominieren. Die Bilder in der DDD+ wurden zufällig aus der Gesamtdatenbank ausgewählt, ohne auf die Komplexität (z.B. Witterung oder Pose der Tiere) zu achten. Die einzigen Voraussetzungen waren die Auswahl von zusammenhängenden Sequenzen und dass sich auf den Bildern nicht Individuen verschiedener Spezies befinden. Das wäre problematisch, weil wir die Ordnerstruktur zum Auslesen der Label benutzen und so zu falschen Labels kommen könnte.

\subsection{Experimentelle Optimierung und Auswertung der SVM mit HOG} \label{sec:HOG_parameter_and_results}
In diesem Kapitel wird die experimentelle Bestimmung der besten Parameter für den HOG-Deskriptor und die SVM behandelt. Für diesen Teil wurden die Tagesbilder von Dachs und Damhirsch verwendet.


\subsubsection{Arten der Testdaten} \label{sssec:test_data_HOG}
Der HOG-Klassifizierer wurde auf einer Datenbank mit bereits bestimmten ROIs verwendet. In ersten Experimenten und zur optimalen Parameterbestimmung wurden Daten mit manuell selektierten ROIs verwendet. Dies wurde getan, um einen möglichen Fehler durch die automatische ROI-Selektion mit PCA auszuschließen. Später wurde die SVM jedoch mit automatisch selektierten ROIs trainiert und klassifiziert.\\ 
Da der Datensatz der Dachsbilder sehr gering war, wurde versucht die Trainingsdatenmenge durch vertikale Spiegelung der Bilddaten künstlich zu erhöhen. Auf zehn Testläufen wurde eine geringe Verbesserung der Ergebnisse bestimmt. Auch wenn diese Erhöhung nicht signifikant war und in seltenen Fällen eine Reduktion der Präzision verursachte, wurde die künstliche Testdatenerhöhung für alle Tests im folgenden Teil angewandt. 

Ein weiteres Problem der geringen Menge der Daten für den Dachs war, dass bei prozentualer Selektion der Trainingsdaten der Großteil aus Damhirschbildern bestand. Wurde die SVM mit dieser unausgewogenen Verteilung der Tierklassen trainiert, konnte die SVM fast nur noch Damhirsche erkennen. Die Erkennungsrate für Dachse lag dabei bei unter 50~\%. Dieses Problem wurde dadurch gelöst, dass darauf geachtet wurde gleich große Mengen an Trainingsdaten zu verwenden. Durch diese Maßnahme wurde die Präzision mit geeigneten Parametern auf durchschnittlich über 90~\% erhöhen. In der Zukunft scheint auch eine Verwendung des Parameters \texttt{class\_weight="balanced"} sinnvoll, um bessere Ergebnisse auf ungleichmäßig verteilten Datenmengen zu erreichen.

\subsubsection{Parameterbestimmung HOG-Deskriptor und Support Vector Machines} \label{sssec:HOG:parmeter}
Zwei der wichtigsten Parameter des HOG-Deskriptors sind die Größe des betrachteten Bildausschnittes und die Einteilung in Zellen, denn diese Größen bestimmen zum einem die benötigte Rechenzeit und zum anderem wie viele Details des Bildausschnittes regional aufgelöst werden. Außerdem ergibt sich aus dieser Kombination die Dimension des Ergebnisvektors. Dieser hat wiederum Einfluss auf den Rechenaufwand der SVM. Ziel der ersten Parameterfindung war es, die Präzision der Klassifizierung zu maximieren. Dabei wurde besonders darauf geachtet, dass die Präzision für beide trainierten Klassen maximal wurde. Dazu wurden die folgenden Bildausschnittgrößen betrachtet: 64 $\times$ 128, 128  $\times$ 256, 64 $\times$ 64, 128 $\times$ 128, 128 $\times$ 64 und 256 $\times$ 128 Pixel². Jede dieser Größen wurde mit einer Zellgröße von 16 $\times$ 16 und 32 $\times$ 32 Pixeln getestet. Für die Blockgröße wurde jeweils die Kombination aus 4 Zellen verwendet. Die Ergebnisse sind in Tabelle~\ref{tab:HOG_parameter_selection} gezeigt.  
Ebenfalls wurde versucht die Parameter der SVM zu optimieren. Dazu wurde die \textit{trainAuto} Methode von OpenCV verwendet. Die besten Parameter waren für die Kostenfunktion $C=12,5$ und $\gamma=0,50625$. Diese Werte liegen sehr nah an den Standardwerten ($C=12$ und $\gamma=0,5$) für die SVM, sodass keine nennenswerten Änderungen an den Werten aus Tabelle~\ref{tab:HOG_parameter_selection} bestimmt werden konnten.

\begin{table}[]
	\centering
	\caption{Präzision der Klassifizierung abhängig von der Bildausschnittgröße und der Zellgröße. Es werden aus 50 zufällig generierten Trainings- und Testdatenreihenfolgen die jeweils erreichte durchschnittliche Präzision $\overline{p}$, die minimale sowie maximale erhaltene Präzision ($p_{min}, p_{max}$) und die Standardabweichung $\sigma$ angegeben. }
	\label{tab:HOG_parameter_selection}
	\begin{tabular}{cl}
		\hline
		\textbf{Ausschnitt [Pixel²]} & \multicolumn{1}{c}{\textbf{Zellgröße 16 $\times$ 16}}                                                                                                                                                  \\ \hline
		\textbf{64 $\times$ 128}                         & \begin{tabular}[c]{@{}l@{}}Dachs: $\overline{p}= 0.913, p_{min}=0.750, p_{max}=1.000, \sigma=0.062$ \\ Damhirsch: $\overline{p}=0.914, p_{min}=0.831, p_{max}=0.987, \sigma=0.034$\end{tabular} \\
		\textbf{128 $\times$ 256}                      & \begin{tabular}[c]{@{}l@{}}Dachs: $\overline{p}=0.488, p_{min}=0.167, p_{max}=0.833, \sigma=0.161$ \\ Damhirsch: $\overline{p}=0.996, p_{min}=0.949, p_{max}=1.000, \sigma=0.011$\end{tabular}          \\
		\textbf{64 $\times$ 64}                           & \begin{tabular}[c]{@{}l@{}}Dachs: $\overline{p}=0.908, p_{min}=0.833, p_{max}=1.000, \sigma=0.045 $\\ Damhirsch: $\overline{p}=0.859, p_{min}=0.797, p_{max}=0.916, \sigma=0.035$\end{tabular}          \\
		\textbf{128 $\times$ 128}                      & \begin{tabular}[c]{@{}l@{}}Dachs: $\overline{p}=0.818, p_{min}=0.500, p_{max}=1.000, \sigma=0.111 $\\ Damhirsch: $\overline{p}=0.951, p_{min}=0.882, p_{max}=0.992, \sigma=0.023$\end{tabular}          \\
		\textbf{128 $\times$ 64}                        & \begin{tabular}[c]{@{}l@{}}Dachs: $\overline{p}=0.933, p_{min}=0.833, p_{max}=1.000, \sigma=0.050 $\\ Damhirsch: $\overline{p}=0.881, p_{min}=0.709, p_{max}=0.979, \sigma=0.069$\end{tabular}          \\
		\textbf{256 $\times$ 128}                      & \begin{tabular}[c]{@{}l@{}}Dachs: $\overline{p}=0.675, p_{min}=0.250, p_{max}=1.000, \sigma=0.308 $\\ Damhirsch: $\overline{p}=0.781, p_{min}=0.367, p_{max}=1.000, \sigma=0.249$\end{tabular}          \\ \hline
		& \\
		\hline
		\textbf{Ausschnitt [Pixel²]} & \multicolumn{1}{c}{\textbf{Zellgröße 32 $\times$ 32}}                                                                                                                                                  \\ \hline
		\textbf{64 $\times$ 128}                         & \begin{tabular}[c]{@{}l@{}}Dachs: $\overline{p}= 0.875, p_{min}=0.750, p_{max}=1.000, \sigma=0.067$ \\ Damhirsch: $\overline{p}=0.854, p_{min}=0.755, p_{max}=0.907, \sigma=0.044$\end{tabular} \\
		\textbf{128 $\times$ 256}                      & \begin{tabular}[c]{@{}l@{}}Dachs: $\overline{p}=0.883, p_{min}=0.750, p_{max}=1.000, \sigma=0.076$ \\ Damhirsch: $\overline{p}=0.894, p_{min}=0.810, p_{max}=0.928, \sigma=0.037$\end{tabular}          \\
		\textbf{64 $\times$ 64}                           & \begin{tabular}[c]{@{}l@{}}Dachs: $\overline{p}=0.975, p_{min}=0.917, p_{max}=1.000, \sigma=0.038 $\\ Damhirsch: $\overline{p}=0.859, p_{min}=0.708, p_{max}=0.574, \sigma=0.072$\end{tabular}          \\
		\textbf{128 $\times$ 128}                      & \begin{tabular}[c]{@{}l@{}}Dachs: $\overline{p}=0.908, p_{min}=0.667, p_{max}=1.000, \sigma=0.102 $\\ Damhirsch: $\overline{p}=0.840, p_{min}=0.705, p_{max}=0.903, \sigma=0.062$\end{tabular}          \\
		\textbf{128 $\times$ 64}                        & \begin{tabular}[c]{@{}l@{}}Dachs: $\overline{p}=0.967, p_{min}=0.917, p_{max}=1.000, \sigma=0.041 $\\ Damhirsch: $\overline{p}=0.834, p_{min}=0.776, p_{max}=0.932, \sigma=0.045$\end{tabular}          \\
		\textbf{256 $\times$ 128}                      & \begin{tabular}[c]{@{}l@{}}Dachs: $\overline{p}=0.892, p_{min}=0.833, p_{max}=1.000, \sigma=0.075 $\\ Damhirsch: $\overline{p}=0.867, p_{min}=0.797, p_{max}=1.000, \sigma=0.954$\end{tabular}          \\ \hline
	\end{tabular}
\end{table}

In den gezeigten Tabellen wird deutlich, dass die Präzision für die beiden Zellgrößen in einem ähnlichem Genauigkeitsbereich liegt. Jedoch scheint eine größere Zellgröße etwas besser geeignet, um Dachse zu erkennen, und etwas schlechter geeignet, um Damhirsche zu erkennen. Die besten Ergebnisse wurden für die Kombination aus einer Bildauschnittgröße von 64 $\times$ 128 Pixel² und einer Zellgröße von 16 $\times$ 16 Pixel² erhalten. Dieser Wert entspricht den üblichen Standardwerten für die Fußgängerdetektion in OpenCV. Für die Tierdetektion ist dieses Ergebnis überraschend, da die Seitenverhältnisse der betrachteten Tiere eher 1:1 bzw. 1:2 entsprechen. Eine Begründung für diese Abweichung kann an dieser Stelle nicht gegeben werden. 

Im folgenden werden Experimente nur noch mit den hier gefundenen optimalen Parametern durchgeführt.

\subsubsection{Ergebnisse}
Die in Kapitel~\ref{sssec:HOG:parmeter} bestimmten optimalen Parameter wurden mit denen durch PCA ermittelten ROIs und der künstlichen Erhöhung der Testdaten getestet. Die in Tabelle~\ref{tab:HOG:ResultsAuto} gezeigten Ergebnisse stammen aus 50 Tests mit den selben Daten, aber einer zufälligen Unterteilung der Daten in Trainings- und Testdaten. Es ist zu erkennen, das die Präzision etwas geringer ist, als mit den manuell ausgewerteten ROIs. Dies war zu erwarten, weil die automatische Bestimmung der ROIs mit PCA kleine Fehler enthält. Der Vorteil dieser Variante ist dennoch die automatische ROI-Selektion und damit eine deutliche Aufwandsreduktion für den Benutzer. 

\begin{table}[]
	\centering
	\caption{Präzision des Klassifizierers der sowohl mit den aus PCA stammenden ROIs trainiert und getestet wurde. Es werden aus 50 zufällig generierten Trainings- und Testdatenreihenfolgen die jeweils erreichte durchschnittliche Präzision $\overline{p}$, die minimale sowie maximale erhaltene Präzision ($p_{min}, p_{max}$) und die Standardabweichung $\sigma$ angegeben. }
	\label{tab:HOG:ResultsAuto}
	\begin{tabular}{cl}
		\hline
		\textbf{Tiergattung} 	       & \multicolumn{1}{c}{\textbf{Präzision}}                                                                                                                                                  \\ \hline
		\textbf{Dachs}                         &$\overline{p}= 0.859, p_{min}=0.761, p_{max}=0.957, \sigma=0.052$ \\
		\textbf{Damhirsch}                &$\overline{p}=0.833, p_{min}=0.763, p_{max}=0.930, \sigma=0.032$ \\
		\hline
	\end{tabular}
\end{table}

Experimentell wurde der HOG-Klassifizierer auch auf eine Datenbank mit Bildern von Damhirsch, Dachs, Wildschein und Schaf angewendet. Mit Erhöhung der Datenklassen sank die Güte der Ergebnisse auf den jeweiligen Klassen deutlich. Die Präzision lag üblicherweise unter 70~\%, für Dachse sogar unter 50~\%. Deshalb halten wir diese Technik ungeeignet für größere und kompliziertere Klassifizierungsprobleme und haben sie nicht weiter auf der DDD+ evaluiert.

\subsection{Auswertung Spatial Pyramid Matching mit LLC}

\subsubsection{Laufzeit}

In der Praxis hat sich die Laufzeit von SPM mit LLC als der limitierende Faktor herausgestellt. Unsere Bildausschnitte, die von PCA bestimmt wurden, werden vor der Berechnung auf eine Größe von 300 Pixeln Seitenlänge oder Höhe verkleinert, um die Anzahl der zu berechnenden Features zu verringern. Trotzdem liegen die Laufzeiten pro Bild momentan in einem Bereich, der eine praktische Anwendung schwierig macht. Alle folgenden Tests wurden auf einem Desktop-PC aus dem Jahr 2014 mit einer i5-4590 CPU mit vier Kernen à 3.3GHz und 8GB RAM durchgeführt.

Wir haben festgestellt, dass der überwiegende Teil der Berechnungszeit für die Berechnung der LLC-Codes verwendet wird. Das Training der SVM mit linearem Kernel und die Berechnung der Features sowie das Training des Codebooks sind wie erwartet schnell. Die genaue Laufzeit ist abhängig von der Größe des Codebook, wobei die Lösung des linearen Gleichungssystems etwa 80 bis 95~\% der Berechnung ausmacht.

Um die Enkodierung zu beschleunigen wurden mehrere Varianten des Algorithmus implementiert. Die erste entspricht einer klassischen sequentiellen Umsetzung mit Numpy ohne zusätzliche Optimierung. In der zweiten Variante haben wir versucht die unabhängige Kodierung der 16 $L_2$-Bins zu parallelisieren. Dafür wurde mit der Funktion \texttt{pool.map} aus dem Modul \texttt{multiprocessing} je ein Thread pro Bin gestartet. Leider hat sich in unseren Tests keine signifikante Verbesserung oder sogar eine Verschlechterung der Laufzeit eingestellt.\\
Zuletzt haben wir die sequentielle Variante wiederhergestellt und die Low-Level-Funktionen mithilfe von Numba automatisch optimieren lassen \cite{numba}. Numba ist eine Bibliothek, die Pythoncode mit einem JIT-Compiler auf Basis des LLVM-Compilers optimiert. Dieser Schritt sorgte für eine deutliche Verbesserung der Laufzeit, die aber leider immer noch nicht zufriedenstellend ist. 

Eine Gegenüberstellung der Ergebnisse befindet sich in Tabelle~\ref{tab:runtime}. Die lange Laufzeit für Codebooks der Größe 2048 hatte zur Folge, dass wir keine Codebooks dieser Größe zur Klassifizierung benutzt haben. Der nächste Schritt wird es sein die Enkodierung mit \emph{Tensorflow} zu programmieren und die Berechnungen der Codes aller Features eines Bilds parallel auf der GPU durchzuführen.

\begin{table}
	\centering
	\begin{tabular}{|l|c|c|c|c|}
		\hline
		\textbf{Variante} & \textbf{n = 256} & \textbf{n = 512} & \textbf{n = 1024} & \textbf{n = 2048} \\ \hline
		sequentiell & 0,44$\pm$0,25s & 1,55$\pm$0,94s & 8,32$\pm$3,99s & 39,92$\pm$20,62s \\ \hline
		multiprocessing & 1,71$\pm$0,19s & 2,57$\pm$0,79s & 9,69$\pm$4,87s & 38,37$\pm$19,46s \\ \hline
		Numba & 0,32$\pm$0,29s & 1,13$\pm$0,66s & 5,96$\pm$2,70s & 24,39$\pm$14,27s \\ \hline
	\end{tabular}
\caption{Gegenüberstellung durchschnittlichen Laufzeiten mit Standardabweichung der drei implementierten Varianten des Algorithmus für verschiedene Größen des Codebooks.}
\label{tab:runtime}
\end{table} 

\subsubsection{Klassifizierung auf DDD}

Dank der Implementierung des \texttt{SpmTransformers} als Scikit-learn-BaseEstimator waren wir in der Lage die Klassifizierung mit Spatial Pyramid Matching und einer linearen SVM problemlos in eine Pipeline einzubetten. Zur Bestimmung der optimalen Parameter haben wir die randomisierte Cross Validation \emph{RandomizedSearchCV} mit fünf Folds benutzt. Hierbei wurden 80~\% der Daten fürs Training und 20~\% zum Testen benutzt. Insgesamt wurden für SIFT- und LBP-Features jeweils zwanzig zufällige Parametersätze getestet.\\
Die Parameterverteilung sah wie folgt aus:
\begin{itemize}
	\item Codebookgröße: [256, 512, 1025]
	\item Alpha: reciprocal(100, 1000)
	\item Sigma: reciprocal(50, 500)
	\item Pooling: ['max', 'sum']
	\item Normalisierung: ['sum', 'eucl']
	\item C: reciprocal(0.1, 5)
\end{itemize} 

Eine reziproke Verteilung bedeutet, dass die Parameter in dem Bereich zufällig bestimmt werden, wobei kleinere Werte mit einer höheren Wahrscheinlichkeit gewählt werden. Der Logarithmus der Verteilung ist ungefähr gleichverteilt.

Als bester Parametersatz für SIFT hat sich eine Codebookgröße von 1024, Alpha = 357, Sigma = 330, Sum-Pooling, euklidische Normalisierung und C = 0,55 herausgestellt. Als Scoring wurde F1-Scoring benutzt, um es zu bestrafen alle Tiere als Damhirsche zu klassifizieren. Die F1-Scores auf den Validation Sets waren sehr vielversprechend:

\begin{center}
\begin{tabular}{|l|r|r|r|r|r|}
	\hline
	\textbf{Fold k = } & 1 & 2 & 3 & 4 & 5 \\ \hline
	\textbf{F1-Score} & 0,9901 & 0,97029 & 0,98 & 1,0 & 0,9583 \\ \hline
\end{tabular}
\end{center}

Auf dem Testdatensatz ergab sich ein gewichteter F1-Score von 0,95493. Alle acht Dachsbilder wurden korrekt klassifiziert. Bei den Damhirschbildern gab es drei Misklassifikationen. 

Auf die Daten für das uniforme LBP-Feature gehen wir nicht im Detail ein. Die Ergebnisse sind etwas schlechter, für die besten Parametersätze aber durchaus überraschend gut, wenn man bedenkt, dass das Feature lediglich 17 Dimensionen hat. Die Varianz zwischen den Ergebnissen war deutlich größer und überraschenderweise nahm die Genauigkeit bei Codebooks der Größe 1024 deutlich ab. Des Weiteren generalisieren die Ergebnisse bei LBP weniger gut. Als Unterstützung für SIFT scheint es aber eine gute Wahl zu sein.

\subsubsection{Klassifizierung auf DDD+}

Die Datenbank DDD+ zeichnet sich durch deutlich kompliziertere Klassen aus. Um ein Gefühl für die Daten und die Parameter zu erhalten. haben wir zunächst einen Test mit SIFT auf wenigen Trainingsdaten mit RandomSearchCV und den gleichen Einstellungen wie im vorherigen Abschnitt durchgeführt. Aus dem kompletten Pool an Daten wurden lediglich 100 Trainings- und 100 Testbilder ausgewählt. Das sorgt für lediglich 12 bis 25 Trainingsbilder pro Klasse.

Bei zehn Durchläufen mit zufälligen Parametersätzen haben wir einen Mean Test Score von 0,43 auf den Trainingsdaten erhalten. Überraschenderweise lag der gewichtete F1-Score auf den Testdaten bei sogar 0,52 also höher als auf den Validation Sets. Diese Ergebnisse sind nicht überragend, aber für die geringe Anzahl an Trainigsdaten und die Komplexität der Aufgabe durchaus überzeugend. Sie decken sich in etwa mit denen von \cite{wyylhg10} auf ähnlich komplexen Daten.

Anschließend haben wir die empirisch bestimmten optimalen Parameter zur Evaluierung des Verfahrens auf der kompletten DDD+ benutzt. Aufgrund der Datenmenge war Cross Validation hier nicht möglich. Wieder werden die Daten im Verhältnis 80:20 auf Trainings- und Testdaten aufgeteilt. Wir wenden LLC auf SIFT-Features mit einem Codebooks der Größe 1024, Alpha = 500, Sigma = 100, Max-Pooling und euklidischer Normalisierung an. Die Klassifikation der Trainingsdaten selbst ergibt als Bias einen gewichteten F1-Score von 0,9853. Das beweist eindeutig die lineare Separierbarkeit der LLC-Codes. Leider fällt das Ergebnis auf den Testdaten mit 0,7678 etwas schlechter, aber nach wie vor gut aus. Möglicherweise lässt sich das Overfitting mit noch stärkerer Regularisierung durch Erhöhung von Alpha verringern. Eine Confusion Matrix für die DDD+ befindet sich in Abbildung~\ref{tab:ddd+}. Überraschenderweise waren die Tests bei denen nur SIFT benutzt wurde besser als bei konkatenierten Codes für SIFT und LBP. Dort betrug der durchschnittliche Score 0,7399 für den besten Klassifikator. Wie bei den Tests von LBP auf der DDD angedeutet könnten sich mit kleineren Codebooks für dieses Feature sogar bessere Ergebnisse erzielen lassen.

\begin{table}
	\centering
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline
		& Dachs & Rotfuchs & Feldhase & Wildschwein & Wildschaf & Damhirsch \\
		\hline
		Dachs & 19 & 0 & 3 & 2 & 0 & 2 \\ \hline
		Rotfuchs & 2 & 45 & 1 & 3 & 1 & 4 \\ \hline
		Feldhase & 2 & 4 & 19 & 0 & 0 & 2 \\ \hline
		Wildschwein & 4 & 6 & 1 & 41 & 7 & 5 \\ \hline
		Wildschaf & 0 & 5 & 2 & 0 & 59 & 6 \\ \hline
		Damhirsch & 1 & 1 & 2 & 2 & 4 & 65 \\
		\hline
	\end{tabular}
	\caption{Confusion Matrix der sechs Tierarten nach Anwendung von LLC-kodierten SIFT- und LBP-Features und einer linearen SVM auf der DDD+.}
	\label{tab:ddd+}
\end{table}

\subsubsection{Zusammenfassung der Ergebnisse}

Zusammenfassend lässt sich sagen, dass LLC mit SPM ordentliche Ergebnisse liefert, auch wenn nur wenige Daten vorliegen. Auf der DDD war unsere Klassifikation der menschlichen ebenbürtig, die in der Regel mit 96~\% angegeben wird. Auch auf der komplexeren DDD+ haben wir zufriedenstellende Ergebnisse erhalten, die durch bessere Parameter eventuell noch gesteigert werden können. Die in der Literatur angegebenen Werte von Alpha = 500 und Sigma = 100 haben auch für uns gute Ergebnisse geliefert \cite{wyylhg10}. Anders als dort angegeben, haben wir keine Unterschiede zwischen den Pooling- und Normalisierungsverfahren feststellen können. Bei den SIFT-Features hat insbesondere die Vergrößerung des Codebooks zu signifikaten Steigerungen in der Genauigkeit geführt. So ließen sich die Scores von 0,656 (256 Visual Words), über 0,7168 (512 Visual Words), bis auf 0,7678 für 1024 Visual Words steigern. Nach einer Reimplementierung des Verfahrens mit Tensorflow ist es denkbar, dass sich mit einer Vergrößerung des Codebooks auf zum Beispiel 2048 Repräsentanten noch bessere Ergebnisse einstellen.